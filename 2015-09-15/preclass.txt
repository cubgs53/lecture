For the questions regarding the Game of Life, you may want to refer
to the simple implementation included in the "life" subdirectory.
If you run "make glider", you can see a small example of running
the glider pattern for a few generations.

0.  How much time did you spend on this pre-class exercise, and when?

    ~1 hour on Saturday. Gave up on timing game of life due to OMP issues.

1.  What are one or two points that you found least clear in the
    9/15 slide decks (including the narration)?

    Not quite sure about the domain decomposition stuff. How do you actually handle boundary values? Does each processor communicate its boundary values to the others then once each one has enough info it calculates its new boundary values? Can you parallelize it better by communicating immediately and working asynchronously on the rest of your subdomain and then only block if you finish before you get bounday info from the other processors?

    For speculative async GoL can you reduce how much work you have to do if you need to back up by takign advantage of the speed at which boundary information propogates into the rest of your subdomain? For instance the corner of your sub-domain isn't affected by outside information at step X until many steps later.

2.  In the basic implementation provided, what size board for the Game
    of Life would fit in L3 cache for one of the totient nodes?  Add a
    timer to the code and run on the totient node.  How many cells per
    second can we update for a board that fits in L3 cache?  For a
    board that does not fit?

    - Each entry in the board is 1 byte.
    - L3 cache on each chip is 15MB
    - N^2 = 15,000,000 - N ~ 3872
    - Half size cache if we consider the storage of current step and previous step (2738)
    - Tried running this on totient but submitted jobs said they couldn't find omp :(, running the long ago centroid code has the same problems...but it runs on the head node

3.  Assuming that we want to advance time by several generations,
    suggest a blocking strategy that would improve the operational
    intensity of the basic implementation.  Assume the board is
    not dilute, so that we must still consider every cell.  You may
    want to try your hand at implementing your strategy (though you
    need not spend too much time on it).

    - Each cell only needs the cells around it to compute what value it will be at the next step
    - Thus you can compute the next value of each cell totally in parallel
    - Block until all cells have been computed then start on the next generation

4.  Comment on what would be required to parallelize this code
    according to the domain decomposition strategy outlined in the
    slides.  Do you think you would see good speedups on one of
    the totient nodes?  Why or why not?

    Code to compute the non-boundary values. Then communication to know what is happening along the boundary and then code to specially compute those boundary values.

    With large enough N you should start to see speedups because of the volume-to-surface ratio gets pretty high. But i'm not sure the communication costs or how exactly we would communicate (MPI?). If we decomposed it into 4 parts it should be pretty efficient because the 4 parts could all be on the same chip and thus sharing the same cache which would mean memory accesses to boundary values would be very fast (except when the board doesn't fit in the cache).

5.  Suppose we want to compute long-range interactions between two
    sets of particles in parallel using the obvious n^2 algorithm in a
    shared-memory setting.  A naive implementation might look like

      struct particle_t {
          double x, y;
          double fx, fy;
      };

      // Update p1 with forces from interaction with p2
      void apply_forces(particle* p1, particle* p2);

      // Assume p is the index of the current processor,
      // part_idx[p] <= i < part_idx[p+1] is the range of
      // particles "owned" by processor p.
      //
      for (int i = part_idx[p]; i < part_idx[p+1]; ++i)
          for (int j = 0; j < npart; ++j)
              apply_forces(particle + i, particle + j);

    Based on what you know about memories and parallel architecture,
    do you see any features of this approach that are likely to lead
    to poor performance?

    - We apply all the particles from a single processor to ALL other particles in sequence
    - This means we are constantly accessing particles that belong to all other processors
    - every processor has to keep all the particles
    - have to access all the memory, not taking advantage of locality?
